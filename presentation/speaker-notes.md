# slide 1

(영상 편집점) 지금부터 발표 시작하겠습니다.

안녕하세요
ELK 스택을 활용해 통계성 데이터 제공하는 API 구현하기
라는 주제로 발표하는 유기성이에요.

여러분들 오늘 제 발표를 편안한 마음으로 들으시면 좋을 것 같아요.
저와 저희 팀원들이 실무에서 몇 개월에 걸쳐서 고민한 내용을 요약해서 20분 만에 소개해드리는 것이기 때문에
제가 하는 말을 한 문장 한 문장 완벽하게 이해하지 못하실 수도 있어요. 그래도 괜찮아요.
편안한 마음으로 저와 함께 통계성 API 를 만드는 여정을 떠난다고 생각하시면 좋겠네요.

그럼 목차를 보면서 내용을 간략하게 소개해볼께요.

# slide 2

오늘 저의 발표는 크게 세 영역으로 이루어져요.
첫번째 요구조건 분석, 두번째 기술을 선택하는 과정, 세번째 구현 과정에서의 고민과 해결책을 공유하기 입니다.
이 중에서 저는 세번째 영역이 핵심이라고 생각하면서 준비했어요.
그래서 오늘 제 발표에서 여러분들이 얻어가시게 될 것은 ELK 에 대한 심도깊은 이해라기 보다는 ELK 를 활용해서 API 구현을 하게 될때
어떤 고난과 역경을 헤쳐나가게 되는지 간접 경험을 하는게 메인 포인트라고 할 수 있죠.

# slide 3

먼저 요구조건 분석부터 시작해볼께요.
요구조건 분석 파트에서는 어떤 서비스인지 소개하고 통계성 API 요구조건을 간략하게 할 거에요.

# slide 4

저희 서비스는 SNS 와 같이 많은 사람들이 컨텐츠를 즐기며 서로 상호작용하는 커뮤니티형 서비스라고 생각하시면 될 것 같아요. 유튜브나 페이스북, 인스타그램 같은 서비스죠.
여기엔 크게 세 종류의 도메인이 있는데
1번 컨텐츠, 2번 컨텐츠를 업로드하는 사용자, 3번 컨텐츠를 즐기는 사용자, 4번 통계 정보를 확인하는 어드민이 있어요.
통게성 API 는 여기 4번 어드민 사용자들이 호출하게 되죠.

# slide 5

통계성 API 요구조건에는 아주 다양한 범주가 있을 수 있는데
이 발표에서는 사용자 통게 정보 API 와 컨텐츠 통계 정보 API 두 가지만 예시로 살펴볼께요.

# slide 6

여기서는 사용자 통계 정보 중에서 "방문자 수" 를 제공하고 있어요.
먼저 왼쪽 아래에 기간을 지정할 수가 있어요. 시작시점과 종료시점
그리고 왼쪽 위에서 total 은 지정한 기간에 상관없이 총 방문자 수를 의미하고, current total 은 지정한 기간 내 방문자 수, previous total 은 바로 이전 기간 내 방문자 수를 보여줘요.
예를 들어서 기간을 8월 23일부터 8월 26일까지로 지정한다면 현재 기간은 8월 23일부터 8월 26일까지가 되고 이전 기간은 8월 20일부터 8월 23일까지가 되는거죠.
change rate 는 이전 기간과 비교했을때 현재 기간의 증감률을 의미합니다.
current total 이 1400 이고 previous total 이 800 이니까 600 만큼 증가하면서 증감율을 75% 가 된거죠.
오른쪽 위에를 보면 지정한 기간 동안에 방문자 수 추이를 그래프로 표현하는 것도 있다는 걸 알 수 있어요.
오른쪽 아래를 보면 사용자 관련해서 어떤 메트릭들이 가능한지 예시로 보여드리고 있죠.

# slide 7

여기에 있는건 컨텐츠 통계 정보 중에서는 "컨텐츠 초회수" 에 대한 설명이에요.
앞에서와 비슷하게 기간을 지정할 수 있고, total, current total, previous total, change rate 숫자를 확인할 수 있고
꺾은선 그래프로 추이를 표현하죠.
컨텐츠 관련해서 이러이러한 메트릭들이 있을 수 있구요.

# slide 8

그 다음에는 이런 요구조건을 만족하기 위해서 어떤 기술을 어떻게 선택하게 됐는지 말씀드릴꺼에요.
여기 기술 선택 영역은 먼저 API 가 어떤 흐름으로 동작하게 되는지 큰 그림을 보여드릴꺼고
그리고 나서는 ELK 를 선택한 이유, ELK 에 대한 간략한 소개를 드릴꺼에요.

# slide 9

전체적인 프로세스를 소개해드릴께요.
기본적으로 마이크로서비스 아키텍쳐라서 여러 개의 서버로 나눠져 있어요.
사용자 관련 API 처리를 하는 User 서비스, 컨텐츠 관련 API 처리를 하는 Content 서비스, 통계성 API 처리를 하는 Analytics 서버
위쪽은 데이터를 수집하는 과정이고 아래쪽은 통계성 API 요청을 처리하는 과정을 나타내고 있다고 보시면 돼요.

컨텐츠 관련해서 사용자들이 어떤 행위를 하면 User 서버, Content 서버로 API 요청이 도달을 하고
User 서버, Content 서버는 로그 전처리 프로세스로 로그를 전송을 해요.
일정한 형식으로 정제한 로그 데이터를 로그 데이터 스토어에 계속 저장을 해 두는 거에요.
이렇게 저장해둔 로그 데이터에 대해서
일정한 시간 단위로 aggregate 쿼리를 실행한 결과를 Analytics 서버가 관장하는 Analytics 데이터베이스에 저장을 해두죠.

그러다가 통계성 API 요청을 Analytics 서버가 이 요청을 수신하게 되고
Analytics 서버는 자신의 데이터베이스인 Analytics 데이터베이스에서 조건에 맞는 통계 정보를 획득해서 API 응답으로 되돌려 보내죠.

API 처리 전체적인 구조는 이런식이죠.

# slide 10

데이터 처리 프로세스라는게 세밀하게 보면 아주 복잡하지만
크게 보면 데이터를 입력받는 input, 입력받은 데이터를 처리하는 processing, 처리 결과를 내보내는 output 구조를 가진다고 정리할 수 있어요.

그리고 이 세 가지 항목은 각각 ELK 의 세 가지 항목에 대응시킬 수 있어요.

# slide 11

ELK 의 E 는 Elasticsearch 를 말하고 Elasticsearch 는 processing 역할을 해요.
L 은 Logstash 를 말하고 Logstash 는 input 역할을 하죠.
K 는 Kibana 를 말하고 Kibana 는 output 역할을 맡고 있어요.

# slide 12

다시 앞에 전체적인 프로세스로 돌아가서 살펴보자면
이 중에서 2번 로그 전송, 3번 로그데이터 저장, 4번 aggregate 하는게 input, processing, output 에 해당하고
이 그림을 조금 바꾸면

# slide 13

아까 여기에 있던 Log Preprocessing 자리에는 Logstash 가 들어가고
Log Data Store 자리에는 Elasticsearch 가 들어가게 된다고 보면 돼요.

# slide 14

그런데 저는 왜 ELK 를 선택했을까요?
데이터 분석 플랫폼을 유명한 Hadoop, Spark 도 있는데 말이죠.

그 이유는 첫번째로 백엔드 메인 환경이 NodeJS 이기 때문이에요.
Hadoop, Spark 는 NodeJS Native API 를 공식적으로 지원하고 있지 않아요.
하지만 ELK 는 Javascript Client 를 공식적으로 지원하고 있죠.

두번째로 search 와 aggregation 이 필요한 경우가 많을 것이라고 예상했어요.
ELK 에는 상당히 많은 aggregate 기능이 구현되어 있고, Elasticsearch 가 서치 엔진이라고 불릴 만큼 탐색에 특화되어 있다고 알고 있어요.

세번째로는 요구조건 만족을 위해서 빅데이터 프로세싱보다는 Analytics, Visualization 에 집중하게 될거라고 생각했어요.
Elastic 진영이 현재까지도 이쪽으로 많은 리서치와 개발을 진행하고 있는 만큼 좋은 선택이 될거라고 판단했어요.

그래서 ELK 를 선택헀습니다.

# slide 15

ELk 각각에 대해서 아주 간략하게 설명하고 넘어가볼께요
먼저 Elasticsearch 는 ...

# slide 16

먼저 Logstash 는 ...

# slide 17

먼저 Kibana 는 ...

# slide 18

이제 세 번째 통계성 API 구현 과정에서 제가 어떤 경험들을 했는지 공유해보려고 해요.
이렇게 8가지로 분류할 수 있을 것 같구요. 하나씩 살펴볼께요.

# slide 19

제일 먼저 셋업, 환경설정이죠.
ELK 는 Elasticsearch, Logstash, Kibana 세 가지의 조합을 말하는건데 사실 이 세 가지는 별도의 프로세스에요.
세 가지를 각각 셋업해서 서로 유기적으로 동작하도록 만드는 것은 여간 쉬운 일이 아니죠.

근데 이렇게 유기적으로 동작하는 여러개의 프로세스가 있다면 도커 컨테이너 묶어서 관리하면 훨씬 편리해지거든요.
이걸 어떻게 할 수 있을까 고민하다가 찾은 docker-elk 라는 훌륭한 깃헙 리포지토리가 있어요.
누구든 손쉽게 ELK 환경을 구축해서 다룰 수 있도록 최소한의 configuration 을 제공하는 오픈소스에요.
ELK 를 처음 접하시는 분들일라면 누구에게든 이걸 추천드릴께요.

# slide 20

docker-elk 를 사용해서 셋업을 할때 가장 애를 먹었던 부분 중 하나가 바로 이건데요
사실은 문서만 잘 읽었으면 겪지 않은 고통이었어요.
문서를 보면 이렇게 두번씩이나 강조를 하고 있는데
docker-compose up 을 실행하기 전에 반드시 setup 을 해줘야 합니다. setup 을 할때 필수적으로 세팅해야하는 최소한 configuration 이 완성되거든요.
docker-elk 를 사용할때는 반드시 setup 을 하고 나서 up 을 한다 이걸 기억해주세요.

# slide 21

전체적인 프로세스에서 로그 전처리 시스템으로 로그를 전송한다고 했죠. 기억나시나요?
이때 전송하는 로그를 어떤 형태로 해야 좋을까 엄청 고민이 됐는데요 바로 2번 로그 전송 부분에 대한 이야기에요.

# slide 22

기본적인 아이디어는 사용자가 어떤 동작을 하면 이벤트 기반 로그가 발생한다, 사용자가 또 어떤 동작을 하면 이벤트 기반 로그가 발생한다 이런건데요
정확하게 말하자면
기존에 구현되어 있던 3번 백엔드에서 API 요청을 수신해서 처리하는 컨트롤러 또는 서비스 레이어에
4번 Logstash 로 로그를 전송하는 로직을 추가하는게 여기서의 메인 태스크라고 할 수 있어요.

# slide 23

이벤트 기반으로 로그를 기록한다는건 어떤 의미일까요?
제가 선택한 로그 형태는 오른쪽과 같아요.
필드명은 자유롭게 바꾸셔도 되구요 event_type 이라는 필드를 명시해서
이번 로그가 사용자의 어떤 동작에 대한 로그인지 기록으로 남기는거죠.

여기서는 sign-in, play-content(컨텐츠 재생), purchase(어떤걸 구매하는 것)을 예시로 들고 있는데요

# slide 24

좀 더 자세히 살펴보자면
event_type 이 purchase 인 경우 user 는 구매한 사용자 아이디, item 는 구매한 아이템 아이디, price 는 구매 시 가격, quantity 는 구매 수량을 의미하죠.
구매 이벤트들을 이런식으로 로그를 남겨놓으면 나중에 user 별로, item 별로 구매에 대한 통계 정보를 확인할 수 있게 되죠.
event_type 이 sign-in 인 경우 user 는 로그인한 사용자, browser 는 어떤 브라우저로 로그인 했는지, country 는 어떤 국가에서 로그인했는지를 의미하죠.
로그인 이벤트도 비슷하게 user 별로, browser 별로, country 별로 로그인 통계 정보를 확인할 수 있죠.

# slide 25



# slide 26

# slide 27

# slide 28

# slide 29

# slide 30

# slide 31

# slide 32

# slide 33

# slide 34

# slide 35

# slide 36

# slide 37

# slide 38

# slide 39

# slide 40

# slide 41

# slide 42

# slide 43

# slide 44

# slide 45

# slide 46

# slide 47

# slide 48

# slide 49

# slide 50

# slide 51

# slide 52

(영상 편집점) 감사합니다. 제가 준비한 발표는 여기까지입니다.

# slide 53
