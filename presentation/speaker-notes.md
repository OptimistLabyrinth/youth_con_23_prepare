# slide 1

 (영상 편집점) 지금부터 발표 시작하겠습니다. 

안녕하세요 ELK 스택을 활용해 통계성 데이터 제공하는 API 구현하기 라는 주제로 발표하는 유기성이에요. 오늘 제 발표는 편안한 마음으로 들으시면 좋을 것 같아요. 저와 제 팀원들이 실무에서 몇 개월에 걸쳐서 고민한 내용을 요약해서 4, 50분 만에 소개해드리는 것이기 때문에 저의 말을 한 문장 한 문장 완벽하게 이해하지 못하실 수도 있어요. 그래도 괜찮아요. 편안한 마음으로 저와 함께 통계성 API 를 만드는 여정을 떠난다고 생각하시면 좋겠네요.
그리고 여러분들께 부탁드리고 싶은게 있습니다. 중간 중간에 생각나는 질문은 잠시 참아주시고 저의 강연이 모두 끝난 뒤에 Q&A 세션에서 제게 질문 주시면 감사하겠습니다.

그럼 목차를 보면서 내용을 간략하게 소개해볼께요. 

# slide 2 

오늘 저의 발표는 크게 세 영역으로 이루어져요. 
첫번째 요구조건 분석, 두번째 기술을 선택하는 과정, 세번째 구현 과정에서의 고민과 해결책 공유하기 이렇게 세 가지 에요. 이 중에서 저는 세번째 영역이 핵심이라고 생각했어요. 그래서 오늘 제 발표를 통해서 여러분들이 얻어가시게 될 것은 ELK 자체에 대한 심도깊은 이해라기 보다는 ELK 를 활용해서 API 구현을 하게 될때 어떤 고난과 역경을 헤쳐나가게 되는지 간접 경험을 하는게 메인 포인트라고 할 수 있죠. 

# slide 3 

먼저 요구조건 분석부터 시작해볼께요. 요구조건 분석 파트에서는 어떤 서비스인지 소개하고 통계성 API 요구조건을 간략하게 소개할 거예요. 

# slide 4 

저희의 서비스는 SNS 와 같이 많은 사람들이 컨텐츠를 즐기며 서로 상호작용하는 커뮤니티형 서비스라고 생각하시면 될 것 같아요. 유튜브나 페이스북, 인스타그램 같은 서비스죠. 여기엔 크게 네 종류의 도메인이 있는데 1번 컨텐츠, 2번 컨텐츠를 업로드하는 사용자, 3번 컨텐츠를 즐기는 사용자, 4번 통계 정보를 확인하는 어드민이 있어요. 통계성 API 는 여기 4번 어드민 사용자들이 호출하게 되죠.

# slide 5 

통계성 API 요구조건에는 아주 다양한 범주가 있을 수 있는데 이 발표에서는 사용자 통계 정보와 콘텐츠 통계 정보 두 가지만 예시로 살펴볼께요. 

# slide 6 

여기서는 사용자 통계 정보 중에서 "방문자 수" 를 살펴볼 거예요. 먼저 왼쪽 아래에 기간을 지정할 수가 있어요. 시작시점과 종료시점을 선택하는 방식으로. 왼쪽 위에 total 은 지정한 기간에 상관없이 총 방문자 수를 의미하고, current total 은 지정한 기간 내 방문자 수, previous total 은 바로 이전 기간 내 방문자 수를 보여줘요. 예를 들어서 기간을 7월 28일부터 7월 29일까지로 지정한다면 현재 기간은 7월 28일부터 7월 29일까지가 되고 이전 기간은 7월 26일부터 7월 27일까지가 되는거죠. change rate 는 이전 기간과 비교했을때 현재 기간의 증감률을 의미합니다. current total 이 1400 이고 previous total 이 800 이니까 600 만큼 증가하면서 증감률은 75% 가 된거죠. 오른쪽 위에를 보면 지정한 기간 동안에 방문자 수 추이를 그래프로 표현하는 것도 있다는 걸 알 수 있어요. 오른쪽 아래에는 사용자 관련해서 어떤 메트릭들이 가능한지 예시로 보여드리고 있죠. 

# slide 7 

여기에 있는건 콘텐츠 통계 정보 중에서는 "콘텐츠 조회수" 에 대한 설명이에요. 앞에서와 비슷하게 기간을 지정할 수 있고, total, current total, previous total, change rate 숫자를 확인할 수 있고 꺾은선 그래프로 추이를 표현하죠. 컨텐츠 관련해서 이러이러한 메트릭들이 있을 수 있구요. 

# slide 8 

두번째 이런 요구조건을 만족하기 위해서 어떤 기술을 어떻게 선택하게 됐는지 말씀 드릴 거에요. 여기 기술 선택 영역은 먼저 API 가 어떤 흐름으로 동작하는지 그림으로 보여드릴 거고 그리고 나서는 ELK 를 선택한 이유를 말씀드릴께요.

# slide 9 

전체적인 프로세스를 보여드릴께요. 
기본적으로 마이크로서비스 아키텍쳐라서 여러 개의 서버로 나눠져 있어요. 사용자 관련 API 처리를 하는 User 서비스, 컨텐츠 관련 API 처리를 하는 Content 서비스, 통계성 API 처리를 하는 Analytics 서버 위쪽은 데이터를 수집하는 과정이고 아래쪽은 통계성 API 요청을 처리하는 과정을 나타내고 있다고 보시면 돼요. 컨텐츠 관련해서 사용자들이 어떤 행위를 하면 User 서버, Content 서버로 API 요청이 도달을 하고 User 서버, Content 서버는 로그 전처리 프로세스로 로그를 전송을 해요. 일정한 형식으로 정제한 로그 데이터를 로그 데이터 스토어에 계속 저장을 해 두는 거에요. 이렇게 저장해둔 로그 데이터에 대해서 일정한 시간 단위로 aggregate 쿼리를 실행한 결과를 Analytics 서버가 관장하는 Analytics 데이터베이스에 저장을 해두죠. 그러다가 통계성 API 요청을 Analytics 서버가 이 요청을 수신하게 되고 Analytics 서버는 자신의 데이터베이스인 Analytics 데이터베이스에서 조건에 맞는 통계 정보를 획득해서 API 응답으로 되돌려 보내죠. API 처리 전체적인 구조는 이런식이죠. 

# slide 10 

그런데 왜 ELK 를 선택했을까요? 데이터 분석 플랫폼을 유명한 Hadoop, Spark 도 있는데 말이죠. 

그 이유는 첫번째로 백엔드 메인 환경이 NodeJS 이기 때문이에요. Hadoop, Spark 는 NodeJS Native API 를 공식적으로 지원하고 있지 않아요. 하지만 ELK 는 Javascript Client 를 공식적으로 지원하고 있죠. 두번째로 search 와 aggregation 이 필요한 경우가 많을 것이라고 예상했어요. ELK 에는 상당히 많은 aggregate 기능이 구현되어 있고, Elasticsearch 가 서치 엔진이라고 불릴 만큼 탐색에 특화되어 있다고 알고 있어요. 세번째로는 요구조건 만족을 위해서 빅데이터 프로세싱보다는 Analytics, Visualization 에 집중하게 될거라고 생각했어요. Elastic 진영이 현재까지도 이쪽으로 많은 리서치와 개발을 진행하고 있는 만큼 좋은 선택이 될거라고 판단했어요. 이런 이유로 ELK 를 선택했습니다. 

# slide 11 

데이터 처리 프로세스라는게 세밀하게 보면 아주 복잡하지만 크게 보면 데이터를 입력받는 input, 입력받은 데이터를 처리하는 processing, 처리 결과를 내보내는 output 구조를 가진다고 정리할 수 있어요. 그리고 이 세 가지 항목은 각각 ELK 의 세 가지 항목에 대응시킬 수 있어요. 

# slide 12 

ELK 의 E 는 Elasticsearch 를 말하고 Elasticsearch 는 processing 역할을 해요. L 은 Logstash 를 말하고 Logstash 는 input 역할을 하죠. K 는 Kibana 를 말하고 Kibana 는 output 역할을 맡고 있어요. 

# slide 13 

다시 앞에 전체적인 프로세스로 돌아가서 살펴보자면 이 중에서 2번 로그 전송, 3번 로그데이터 저장, 4번 aggregate 하는게 input, processing, output 에 해당하고 이 그림을 조금 바꾸면 

# slide 14 

아까 여기에 있던 Log Preprocessing 자리에는 Logstash 가 들어가고 Log Data Store 자리에는 Elasticsearch 가 들어가게 된다고 보면 돼요. 

그리고 사실 딱 이 위치 2번 맨 마지막, 3번 시작하기 직전에 간단하게 ELK 에 대해서 설명하는게 아주 적절하긴 해요. 그러나 제가 발표 초심자이다보니 발표 내에서 말씀드리고 싶은 것은 끝도 없이 넘치는데 제한된 시간 내에서 하려다보니 ELK 자체에 대한 설명은 생략하기로  결정했어요. 우선 제가 아니더라도 ELK 에 대해서 친절하고 정확하게 설명해주는 글과 영상은 세상에 넘치거든요. 그리고 결정적으로는 제 발표를 들을 때 ELK 에 대한 지식이 조금 부족하더라도 전혀 문제될 것이 없다고 판단이 되더라구요. 그래서 선택과 집중, 제가 공유하고 싶은 내용에 집중하기로 결정했습니다. 

# slide 15 

이제 세 번째 통계성 API 구현 과정에서 제가 어떤 경험들을 했는지 공유해보려고 해요. 이렇게 8가지로 분류할 수 있을 것 같구요. 하나씩 살펴 볼께요. 

# slide 16 

제일 먼저 셋업, 환경설정이죠. ELK 는 Elasticsearch, Logstash, Kibana 세 가지의 조합을 말하는건데 사실 이 세 가지는 별도의 프로세스에요. 세 가지를 각각 셋업해서 서로 유기적으로 동작하도록 만드는 것은 여간 쉬운 일이 아니죠. 근데 이렇게 유기적으로 동작하는 여러개의 프로세스가 있다면 도커 컨테이너 묶어서 관리하면 훨씬 편리해 질 거든요. 이걸 어떻게 할 수 있을까 고민하다가 docker-elk 라는 훌륭한 깃헙 리포지토리를 찾았어요. 누구든 손쉽게 ELK 환경을 구축해서 다룰 수 있도록 최소한의 configuration 을 제공하는 오픈소스에요. ELK 를 처음 접하시는 분들일라면 누구에게든 이걸 추천드릴께요. 

# slide 17 

docker-elk 를 사용해서 셋업을 할때 가장 애를 먹었던 부분 중 하나는 사실 문서만 잘 읽었으면 겪지 않은 고통이었어요. 문서를 작성한 사람들은 이렇게 두번씩이나 강조를 하고 있는데요 docker-compose up 을 실행하기 전에 반드시 setup 을 해줘야 합니다. setup 을 할때 필수적으로 세팅해야하는 최소한 configuration 이 완성되거든요. docker-elk 를 사용할때는 반드시 setup 을 하고 나서 up 을 한다 이걸 기억해주세요. 

# slide 18 

전체적인 프로세스에서 로그 전처리 시스템으로 로그를 전송한다고 했죠. 기억나시나요? 이때 전송하는 로그를 어떤 형태로 해야 좋을까 엄청 고민이 됐는데요 바로 2번에 대한 이야기에요. 

# slide 19 

기본적인 아이디어는 사용자가 어떤 동작을 하면 이벤트 기반 로그가 발생한다, 사용자가 또 어떤 동작을 하면 이벤트 기반 로그가 발생한다 이런건데요 정확하게 말하자면 기존에 구현되어 있던 3번 백엔드에서 API 요청을 수신해서 처리하는 컨트롤러 또는 서비스 레이어에 4번 Logstash 로 로그를 전송하는 로직을 추가하는게 여기서의 메인 태스크라고 할 수 있어요. 

# slide 20 

이벤트 기반으로 로그를 기록한다는건 어떤 의미일까요? 제가 선택한 로그 형태는 오른쪽과 같아요. 필드명은 자유롭게 바꾸셔도 되구요 event_type 이라는 필드를 명시해서 이번 로그가 사용자의 어떤 동작에 대한 로그인지 기록으로 남기는거죠. 여기서는 sign-in, play-content(컨텐츠 재생), purchase(어떤걸 구매하는 것)을 예시로 들고 있는데요 

# slide 21 

좀 더 자세히 살펴보자면 event_type 이 purchase 인 경우 user 는 구매한 사용자 아이디, item 는 구매한 아이템 아이디, price 는 구매 시 가격, quantity 는 구매 수량을 의미하죠. 구매 이벤트들을 이런식으로 로그를 남겨놓으면 나중에 user 별로, item 별로 구매에 대한 통계 정보를 확인할 수 있게 되죠. 
event_type 이 sign-in 인 경우 user 는 로그인한 사용자, browser 는 어떤 브라우저로 로그인 했는지, country 는 어떤 국가에서 로그인했는지를 의미하죠. 로그인 이벤트도 user 별로, browser 별로, country 별로 로그인 통계 정보를 확인할 수 있죠. 

# slide 22 

그 다음에는 로그 데이터를 저장하는 과정에 생겼던 고민에 대해서 말해 볼께요. 

# slide 23 

먼저 elasticsearch 용어를 간단하게 정리하고 넘어갈께요. document 는 개별 데이터 묶음을 의미하고 RDBMS 에서 튜플, 레코드와 비슷해요. index 는 여러 도큐먼트 묶음을 의미하고 RDBMS 에서 테이블과 비슷해요. 

# slide 24 

elasticsearch 에 대해 얘기할때 NoSQL 이고 스키마가 단순하게 설명하는 경우도 많아요. 그러나 elasticsearch 내부적으로는 mapping 이라는 엄격한 타입 시스템을 사용하고 있고 이 mapping 시스템은 앞에서 말한 index, elasticsearch index 단위로 동작해요. 이 부분을 언급하는 이유는 개발 과정에서 예를 들어서 "aaa" 라는 이름의 특정한 필드를 boolean 타입처럼 사용하다가 text, keyword 타입으로 바꿔서 사용해야 하는 경우가 있었어요. 이때 계속해서 타입 불일치 오류가 발생했는데 이 오류를 해결하려면 인덱스 전체를 갱신하는 reindex 를 실행하거나 기존 데이터 전체를 새로운 index 로 복제하는 작업을 해야했어요. 

# slide 25 

이러한 현상을 방지하려면 모든 document 를 하나의 거대한 index 에 생성하기 보다는 index 이름을 조금씩 바꿔가면서 rolling 해서 사용하는 방식이 좋다는걸 뒤늦게야 깨달았어요. 예를 들면 analytics_index 라는 이름을 사용하는 대신에 analytics_index_ 뒤에 무작위 문자열을 붙인다거나 날짜를 붙인다거나 해서 일정한 시간 단위로 index 이름을 바꿔주는거죠. 저장할때는 이렇게 저장하고 나서 나중에 타겟 인덱스는 wildcard 로 지정하면 정상적으로 쿼리를 실행할 수 있어요.

# slide 26 

그 다음 주제는 이쪽 4번 데이터 스토어에 저장해둔 데이터를 수집하는 과정에 대한 거에요.

# slide 27 

데이터를 분류하는 방법은 아주 다양하지만 여기서는 Frequency 데이터와 Recency 데이터라는 관점에서 이야기를 해볼께요. Frequency 데이터는 특정 기간 내에 어떤 정보가 몇 개나 있는지 개수를 세는 것이고 Recency 데이터는 특정 기간 내에서 어떤 정보가 얼마나 최근에 기록되었는지를 확인하는 것이에요. 왼쪽의 표를 보면 기간 내에 빨간색은 5개, 노란색은 2개 검정색은 22개, 하얀색은 18개가 선택되었다는걸 알 수 있어요. 오른쪽의 표를 보면 최근 2일 내에는 1700개, 최근 2~7일 내에는 65개 항목이 선택되었다는걸 의미하죠.

# slide 28 

이런 Frequency 데이터, Recency 데이터 관점을 앞에서 얘기했던 로그 데이터에 적용한다면 어떤 식으로 될까요??

# slide 29 

여기 왼쪽은 elasticsearch 쿼리 예시를 가져온 거에요. 제가 앞으로 가져오는 모든 elasticsearch 쿼리 예시에서 JSON 오브젝트 안에서 쌍따옴표로 표시된 항목은 사용자가 지정하는 커스텀 스트링이고 쌍따옴표 없이 표시된 항목은 elasticsearch 예약어라고 이해해주세요. 여기서 aggs 는 aggregation 쿼리의 시작을 의미하고 terms 는 group_by 를 의미하고 field 는 group_by 의 기준이 되는 항목을 의미하죠. 여기 왼쪽에 있는 쿼리를 해석하면 ‘event_type 별로 항목의 개수를 확인하겠어’ 라는 의미가 되는거죠. 오른쪽은 쿼리를 실행한 결과를 보여주고 있어요. aggregations 는 aggregation 결과의 시작을 의미하고 terms 의 결과는 buckets 에 key, value 쌍으로 담겨 있어요. 각각의 항목을 살펴보면 event_type 이 content-play 인 항목은 242개 있고, event_type 이 sign-in 인 항목은 59개 있고, event_type 이 purchase 인 항목은 4개 있다는 의미이죠. Frequency 정보를 얻는 가장 간단한 쿼리는 이렇다고 정리할 수 있겠습니다.

# slide 30 

좀 더 복잡한 쿼리를 하나 살펴보고 넘어갈께요.
SQL 에서 where 조건절을 입력하듯이 elasticsearch 에서도 조건문을 입력할 수 있는데요, 여기 query 라는 필드를 사용하면 돼요. term 쿼리는 텍스트 완전 일치 탐색을 의미해요. 그리고 elasticsearch 에서는 aggregation 안에 aggregation 을 명시하는 nested aggregation 이 가능합니다. Aggregation 중첩 레벨 제한이 얼마인지 정확히는 모르겠지만 실무에서 사용하는 수준이라고 생각하면 필요한 만큼 할 수 있다고 보시면 될꺼에요. 왼쪽 사진 속 쿼리는 item 별로 group_by 하는 aggregation 결과에 대해 quantity 필드의 값을 sum 하는 aggregation 을 적용하고 있는거죠. 여기 쿼리의 의미는 여기에 써놓은 것처럼 event_type 이 purchase 인 항목들을 item 별로 수량의 합을 구한다는 거에요.
그 결과 오른쪽 사진처럼 sum_quantity.value 값으로 각 item 별 수량의 합을 알 수 있게 되죠.

# slide 31 

그러면 elasticsearch 에서 Recency 정보를 얻어내고 싶다면 어떤 방식으로 해야할까요?? 여러가지 방법이 있는데 그 중에서 top_hits 와 max aggregation 을 추천드리고 싶네요. top_hits aggregation 을 사용하는 경우 latest_document_for_each 처럼 쿼리를 실행하면 돼요. 이 부분을 해석해보면 timestamp 필드를 descending 내림차순 정렬한 뒤 size 1 맨 위에 한 개의 도큐먼트만 반환하고 _source 이건 SQL 에서 SELECT 구문과 비슷하므로 event_type 과 timestamp 필드를 보여달라고 지정하는 거에요. max aggregation 을 사용하느 경우 latest_timestamp_for_each 처럼 실행하는데요, 이건 timestamp 필드를 대소비교해서 큰 값을 얻어내겠다는 거죠.

# slide 32 

왼쪽의 쿼리를 실행하면 오른쪽처럼 결과가 나오게 돼요. 여기 보면 hits 라는 배열 안에 _source 항목에 왼쪽 쿼리에서 선택했던 event_type 과 timestamp 가 들어있죠. 그리고 아래를 보면 value 값에 가장 최근 항목의 timestamp 값이 담겨있죠. 이런식으로 Recency 정보를 다룰 수 있게 됩니다.

# slide 33 

그 다음으로 다룰 주제는 elasticsearch 쿼리 결과에 대한 캐시를 만드는 과정에 대한 것이에요.

# slide 34 

통계성 API 요청 시 시작시점과 종료시점을 항상 지정하게 되죠. 통계성 정보의 시간 범위가 커지면 elasticsearch 에서 직접 쿼리를 실행하는 동작에 부하가 증가한다고 예상할 수 있죠. 그래서 별도의 Analytics 전용 데이터베이스를 두고 한 시간 단위 또는 하루 단위로 elasticsearch aggregation 쿼리 결과를 저장하는 게 좋을 듯 하다고 판단했고 이런식으로 구현을 했어요. 사실 이 Analytics 데이터베이스의 스키마 설계를 어떻게 해야 좋을까 이 부분도 굉장히 많이 고민했던 부분 중에 하나였어요. 하지만 이 부분은 오늘 말고 나중에 기회가 된다면 그때 공유하는 걸로 할께요.

# slide 35 

이렇게 쿼리 결과를 캐싱하는 아이디어는 좋지만 메트릭 속성에 따라서는 적용할 수 없는 경우도 있었어요. 여기서는 unique visitor 를 예시로 들어서 설명드려 볼께요. 8월 25일에 방문자 리스트가 A, B, C, D, B, A 라면 8월 25일의 unique visitor 숫자는 4가 돼요. 8월 26일의 방문자 리스트는 A, B, E, A, E 가 되고 unique visitor 숫자는 3이 되죠. 그리고 8월 25일부터 8월 26일의 방문자 리스트를 보고 unique visitor 값을 확인하려고 하면 8월 25일의 값 4, 8월 26일의 값 3 을 더한 7 이 되는게 아니라 5 가 올바른 값이 되죠. 그러므로 엘라스틱 쿼리 캐시는 요구조건을 아주 면밀히 살펴보면서 적절한 곳에만 적용해야 해요.

# slide 36 

그 다음으로 다중 타임존 지원에 대한건 여기 영역에 해당하는 이야기에요. 통계성 API 호출부터 데이터베이스 쿼리를 실행해서 응답으로 돌려주는 부분까지

# slide 37 

꺾은선그래프로 통계 정보 추이를 표시하는 요구사항이 있는데 이 그래프에서 X축은 언제나 사용자가 현재 위치하고 있는 로컬 타임존에 맞춰서 날짜를 표시해야 한다는 요구사항이 있었어요.
이걸 구현할때 주의할 점이 하나 있죠. 이미 아시는 분들은 다 아시겠지만 날짜 데이터를 데이터베이스에 저장할때는 항상 특정한 시간대로 고정해서 데이터를 다루는게 좋죠. 이 기준점이라는 UTC 가 될 수도 있고 지금 저희가 있는 Asia/Seoul 타임존이 될 수도 있구요.

# slide 38 

UTC 와 로컬타임에 대해서 간단하게 설명을 드리자면 UTC 는 전세계의 시간의 축에서 기준이 되는 지점이고 영점이라고 생각하시면 돼요. 그리고 각 타임존마다 시간 오프셋이 존재하죠. 예를 들어서 어떤 사람이 Asia/Seoul 타임존 즉 오프셋이 +9 인 타임존에 있는데 Asia/Seoul 에 있는 사람에게 8월 26일 15시 정각이라면 UTC 시간으로 8월 25일 06시가 되죠. 또 다른 어떤 사람은 America/Los_Angeles 타임존, 오프셋이 -7 인 타임존에 있는데 로스 앤젤레스에 있는 사람 입장에서 8월 26일 15시라면 UTC 시간으로 8월 26일 22시가 되죠. 이런식으로 같은 날짜 같은 시간이더라도 날짜 데이터를 UTC 값으로 변환하면 서로 다른 값이 되요. 

# slide 39 

이런 논리를 확장하면 하루, 즉 24시간의 개념도 달라지죠. UTC 기준으로 8월 26일 00시부터 8월 27일 00시 까지가 하루가 되겠죠. 그런데 이 하루가 Asia/Seoul 에 있는 사람 입장에서는 8월 25일 15시부터 8월 26일 15시가 되고 America/Log_Angeles 에 있는 사람에게는 8월 26일 07시부터 8월 27일 07시가 되고 Europe/Athens 에 있는 사람에게는 8월 25일 21시부터 8월 26일 21시가 되는거에요.

# slide 40 

다시 요구조건으로 돌아가서 꺾은선그래프 API 를 구현할때 응답 형태는 string 타입인 date, number 타입인 value 객체의 배열의 형태가 되는데요 여기서 date 는 API 를 요청하는 사용자가 있는 로컬 타임 기준으로 표시를 하게 돼요. 우선 저희 팀은 스크럼에서 프론트엔드는 UI 로직에 집중하고 다양한 변환 로직은 백엔드에서 구현하기로 약속했어요. 그러므로 데이터베이스에 날짜 데이터는 전부 UTC 기준으로 저장이 되어 있지만 최종적으로 SELECT 결과에는 통계성 API 를 호출하는 클라이언트의 로컬 타임존 기준으로 표시를 해야 하는거죠. 이걸 어떻게 구현하면 좋을까요??

# slide 41 

많은 데이터베이스에는 timestamp 값에 타임존을 적용한 뒤 문자열로 출력하는 함수가 내장되어 있어요. MySQL 의 경우에는 DATE_FORMAT 이고, PostgreSQL 에서는 TO_CHAR, MongoDB 에서는 dateToString 가 그 역할을 하게 되죠. 여기 Y, M, D, H, M, S 모양으로 포맷을 지정하면 그 형태에 맞춰서 문자열로 응답하는 방식이에요.

# slide 42 

네, 이제 두 개의 장만 남았는데요, 지금까지 앞에 6개 항목은 API 를 구현하면서 생기는 디테일한 부분을 다뤘다면 앞으로 남은 두 장은 보다 일반론적인 이야기를 하게 될 듯 하네요. 데이터 품질 관리와 API 서버 Fallback 처리가 주제에요.

# slide 43 

데이터 분석 및 처리 관련해서 Data Strategy 라는 용어가 있는데요, Data Strategy 에는 이렇게 수많은 데이터 관련 항목들이 존재해요. 데이터 품질 관리라는건 이 거대한 틀을 이루는 요소 중에 하나구요. 

# slide 44 

데이터 품질 관리 프로세스는 다음과 같이 네 단계로 나눠서 생각할 수 있어요. 첫번째 Requirements 에서는 데이터를 평가하는 척도를 정립하는 거구요, 두번째 Assessment 는 앞에서 세운 기준점을 바탕으로 실제 데이터를 평가하게 돼요. 세번째 Issues 에서는 평가 과정에서 발견한 오류를 정리하고 분석하는 과정이 되고 네번째 Monitor and Control 에서는 데이터를 지속적으로 관찰하면서 최대한 잘 정제하는 과정을 거치죠. 이 네 가지가 데이터 품질 관리의 기본적인 흐름이고 데이터를 굉장히 의미있고 소중하게 생각해야만 하는 조직에서는 이런 프로세스 자체를 자동화하기까지 한다고 들었어요.

# slide 45 

데이터 분석, 데이터 관리, 데이터 처리 등의 영역에 대해서 상당히 많은 지식이 쌓여 있는데요, 데이터 품질 관련해서 이런 여섯가지 요소가 중요하다고 알려져 있어요. 
데이터를 다룰 때는 이런 요소들을 고려하시면 더 나은 애플리케이션을 만들 수 있을 거예요. 

# slide 46 

그 다음에 말씀드릴 부분은 API 서버 중단 시 대처법 즉 API 서버 Fallback 에 대해서 말씀드리려고 해요. 사실 이 부분은 통계성 API 를 구현할때만 생기는 고민은 아니구요 개발자라면 프론트엔드, 백엔드 상관없이 모두가 반드시 하게 되는 고민이죠. API 서버에 치명적인 오류가 발생해서 서버 자체가 다운되는 현상이 발생한다면 어떻게 해야할까요??

# slide 47 

여러가지 방법이 있겠지만 제가 생각할 수 있는 선에서 최선의 방식은 이렇습니다. 첫번째, 중단된 API 서버에 발생하는 모든 트래픽에 대해서 리버스 프록시 또는 로드밸런서에서 별도의 서비스 중단 안내페이지로 리디렉션을 해요. 두번째, 오류를 최대한 빨리 해결하고 서비스를 다시 정상적으로 운영 가능한 상태로 만들어요. 세번째 리버스 프록시 설정을 되돌려서 트래픽을 다시 원래의 API 서버로 보냅니다. 이렇게 하면 API 서버 복구가 완료되는 거죠. 물론 이 세 가지 각각도 세부적으로 파고들면 아주 다양한 패러다임이 있고 수많은 아키텍처 패턴이 있겠지만 큰 틀에서는 이런 플로우를 따르게 될 거라고 생각해요.

# slide 48 

제가 너무 시종일관 진지하기만 했던건 아닐까 좀 더 유머러스한 요소를 추가했어야 하는건 아닐까 아쉬움도 남는데요. 그래도 오늘 들은 내용이 머릿속 어딘가에 남아있다가 언젠가 여러분들이 필요한 순간에 번뜩 떠오른다면 저는 그걸로 아주 큰 보람을 느낄 수 있을 듯 합니다. 

이 발표의 PPT 를 다시 보고 싶으시다면 제 깃헙의 youth_23_prepare 리포지토리로 오셔서 presentation 디렉토리로 이동하시면 됩니다. 여기에 링크를 남겨두었으니 링크를 클릭해서 PPT 를 확인하실 수 있을거에요.

(영상 편집점) 제가 준비한 발표는 여기까지입니다. 감사합니다. 

지금부터는 제게 질문을 자유롭게 해주세요.

# slide 49 

# slide 50
